{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started once text preprocessing and vector database setup was finished :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (2.0.15)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (0.1.145)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (2.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.1)\n",
      "Using cached pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.19\n",
      "    Uninstalling pydantic-1.10.19:\n",
      "      Successfully uninstalled pydantic-1.10.19\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.96.0 requires pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2, but you have pydantic 2.10.3 which is incompatible.\n",
      "localai 0.0.26 requires sentence-transformers==2.2.2, but you have sentence-transformers 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-2.10.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.3.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (2.0.15)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (0.3.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (0.3.21)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (0.1.145)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (2.10.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (2.27.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==0.28) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==0.28) (4.67.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai==0.28) (3.10.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai==0.28) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai==0.28) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai==0.28) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.28) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickle5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.0.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.5.20)\n",
      "Requirement already satisfied: build>=1.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (2.10.3)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (0.96.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (4.67.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (0.13.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Collecting pydantic>=1.9 (from chromadb)\n",
      "  Using cached pydantic-1.10.19-cp310-cp310-macosx_11_0_arm64.whl.metadata (152 kB)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.2)\n",
      "Requirement already satisfied: requests-oauthlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.26.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: exceptiongroup in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Using cached pydantic-1.10.19-cp310-cp310-macosx_11_0_arm64.whl (2.3 MB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.3\n",
      "    Uninstalling pydantic-2.10.3:\n",
      "      Successfully uninstalled pydantic-2.10.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.8 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.19 which is incompatible.\n",
      "langchain-core 0.3.21 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.19 which is incompatible.\n",
      "localai 0.0.26 requires sentence-transformers==2.2.2, but you have sentence-transformers 3.3.1 which is incompatible.\n",
      "pydantic-settings 2.6.1 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-1.10.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasa\n",
      "  Downloading rasa-3.6.20-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting CacheControl<0.13.0,>=0.12.9 (from rasa)\n",
      "  Downloading CacheControl-0.12.14-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting PyJWT<3.0.0,>=2.0.0 (from PyJWT[crypto]<3.0.0,>=2.0.0->rasa)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting SQLAlchemy<1.5.0,>=1.4.0 (from rasa)\n",
      "  Downloading sqlalchemy-1.4.54.tar.gz (8.5 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py<1.5,>=0.9 (from rasa)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting aio-pika<8.2.4,>=6.7.1 (from rasa)\n",
      "  Downloading aio_pika-8.2.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting aiogram<2.26 (from rasa)\n",
      "  Downloading aiogram-2.25.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting aiohttp<3.10,>=3.9.0 (from rasa)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting apscheduler<3.10,>=3.6 (from rasa)\n",
      "  Downloading APScheduler-3.9.1.post1-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting attrs<22.2,>=19.3 (from rasa)\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting boto3<2.0.0,>=1.26.136 (from rasa)\n",
      "  Downloading boto3-1.35.76-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (2024.8.30)\n",
      "Collecting cloudpickle<2.3,>=1.2 (from rasa)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: colorclass<2.3,>=2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (2.2.2)\n",
      "Requirement already satisfied: coloredlogs<16,>=10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (15.0.1)\n",
      "Collecting colorhash<1.3.0,>=1.0.2 (from rasa)\n",
      "  Downloading colorhash-1.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting confluent-kafka<3.0.0,>=1.9.2 (from rasa)\n",
      "  Downloading confluent_kafka-2.6.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (51 kB)\n",
      "Requirement already satisfied: cryptography>=41.0.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (43.0.3)\n",
      "Collecting dask==2022.10.2 (from rasa)\n",
      "  Downloading dask-2022.10.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dnspython==2.3.0 (from rasa)\n",
      "  Downloading dnspython-2.3.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting fbmessenger<6.1.0,>=6.0.0 (from rasa)\n",
      "  Downloading fbmessenger-6.0.0-py2.py3-none-any.whl.metadata (869 bytes)\n",
      "Requirement already satisfied: google-auth<3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (2.36.0)\n",
      "Collecting joblib<1.3.0,>=0.15.1 (from rasa)\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jsonpickle<3.1,>=1.3 (from rasa)\n",
      "  Downloading jsonpickle-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema<4.18,>=3.2 (from rasa)\n",
      "  Downloading jsonschema-4.17.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting matplotlib<3.6,>=3.1 (from rasa)\n",
      "  Downloading matplotlib-3.5.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting mattermostwrapper<2.3,>=2.2 (from rasa)\n",
      "  Downloading mattermostwrapper-2.2.tar.gz (2.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting networkx<2.7,>=2.4 (from rasa)\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting numpy<1.25.0,>=1.19.2 (from rasa)\n",
      "  Downloading numpy-1.24.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting packaging<21.0,>=20.0 (from rasa)\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pluggy<2.0.0,>=1.0.0 (from rasa)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from rasa)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting prompt-toolkit<3.0.29,>=3.0 (from rasa)\n",
      "  Downloading prompt_toolkit-3.0.28-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf<4.23.4,>=4.23.3 (from rasa)\n",
      "  Downloading protobuf-4.23.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Collecting psycopg2-binary<2.10.0,>=2.8.2 (from rasa)\n",
      "  Downloading psycopg2_binary-2.9.10-cp310-cp310-macosx_14_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting pydantic<1.10.10 (from rasa)\n",
      "  Downloading pydantic-1.10.9-cp310-cp310-macosx_11_0_arm64.whl.metadata (147 kB)\n",
      "Collecting pydot<1.5,>=1.4 (from rasa)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting pykwalify<1.9,>=1.7 (from rasa)\n",
      "  Downloading pykwalify-1.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pymongo<4.4,>=3.8 (from pymongo[srv,tls]<4.4,>=3.8->rasa)\n",
      "  Downloading pymongo-4.3.3-cp310-cp310-macosx_10_9_universal2.whl.metadata (8.6 kB)\n",
      "Collecting python-dateutil<2.9,>=2.8 (from rasa)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting python-engineio!=5.0.0,<6,>=4 (from rasa)\n",
      "  Downloading python_engineio-4.10.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting python-socketio<6,>=4.4 (from rasa)\n",
      "  Downloading python_socketio-5.11.4-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pytz<2023.0,>=2019.1 (from rasa)\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (6.0.1)\n",
      "Collecting questionary<1.11.0,>=1.5.1 (from rasa)\n",
      "  Downloading questionary-1.10.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting randomname<0.2.0,>=0.1.5 (from rasa)\n",
      "  Downloading randomname-0.1.5.tar.gz (36 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rasa-sdk<3.7.0,>=3.6.2 (from rasa)\n",
      "  Downloading rasa_sdk-3.6.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting redis<5.0,>=4.5.3 (from rasa)\n",
      "  Downloading redis-4.6.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting regex<2022.11,>=2020.6 (from rasa)\n",
      "  Downloading regex-2022.10.31-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.23 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (2.32.2)\n",
      "Collecting rocketchat_API<1.31.0,>=0.6.31 (from rasa)\n",
      "  Downloading rocketchat_API-1.30.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting ruamel.yaml<0.17.22,>=0.16.5 (from rasa)\n",
      "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sanic<21.13,>=21.12 (from rasa)\n",
      "  Downloading sanic-21.12.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sanic-cors<2.1.0,>=2.0.0 (from rasa)\n",
      "  Downloading Sanic_Cors-2.0.1-py2.py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting sanic-jwt<2.0.0,>=1.6.0 (from rasa)\n",
      "  Downloading sanic_jwt-1.8.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sanic-routing<0.8.0,>=0.7.2 (from rasa)\n",
      "  Downloading sanic_routing-0.7.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting scikit-learn<1.2,>=0.22 (from rasa)\n",
      "  Downloading scikit_learn-1.1.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (10 kB)\n",
      "Collecting scipy<1.11.0,>=1.10.0 (from rasa)\n",
      "  Downloading scipy-1.10.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (53 kB)\n",
      "Collecting sentry-sdk<1.15.0,>=0.17.0 (from rasa)\n",
      "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting setuptools>=65.5.1 (from rasa)\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sklearn-crfsuite<0.4,>=0.3 (from rasa)\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting slack-sdk<4.0.0,>=3.19.2 (from rasa)\n",
      "  Downloading slack_sdk-3.33.5-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting structlog<24.0.0,>=23.1.0 (from rasa)\n",
      "  Downloading structlog-23.3.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting structlog-sentry<3.0.0,>=2.0.2 (from rasa)\n",
      "  Downloading structlog_sentry-2.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting tarsafe<0.0.5,>=0.0.3 (from rasa)\n",
      "  Downloading tarsafe-0.0.4-py3-none-any.whl.metadata (858 bytes)\n",
      "Collecting tensorflow-macos==2.12.0 (from rasa)\n",
      "  Downloading tensorflow_macos-2.12.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow_hub<0.14.0,>=0.13.0 (from rasa)\n",
      "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting terminaltables<3.2.0,>=3.1.0 (from rasa)\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.31 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (4.67.0)\n",
      "Collecting twilio<8.3,>=6.26 (from rasa)\n",
      "  Downloading twilio-8.2.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (4.12.2)\n",
      "Collecting typing-utils<0.2.0,>=0.1.0 (from rasa)\n",
      "  Downloading typing_utils-0.1.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting ujson<6.0,>=1.35 (from rasa)\n",
      "  Downloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.3 kB)\n",
      "Collecting webexteamssdk<1.7.0,>=1.1.1 (from rasa)\n",
      "  Downloading webexteamssdk-1.6.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting websockets<11.0,>=10.0 (from rasa)\n",
      "  Downloading websockets-10.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: wheel>=0.38.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rasa) (0.43.0)\n",
      "Requirement already satisfied: click>=7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dask==2022.10.2->rasa) (8.1.7)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dask==2022.10.2->rasa) (2024.10.0)\n",
      "Collecting partd>=0.3.10 (from dask==2022.10.2->rasa)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting toolz>=0.8.2 (from dask==2022.10.2->rasa)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.12.0->rasa) (24.3.25)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading h5py-3.12.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting numpy<1.25.0,>=1.19.2 (from rasa)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.12.0->rasa) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.12.0->rasa) (1.67.1)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.13,>=2.12.0 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiormq~=6.4.0 (from aio-pika<8.2.4,>=6.7.1->rasa)\n",
      "  Downloading aiormq-6.4.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: yarl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aio-pika<8.2.4,>=6.7.1->rasa) (1.17.1)\n",
      "INFO: pip is looking at multiple versions of aiogram to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiogram<2.26 (from rasa)\n",
      "  Downloading aiogram-2.25.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Downloading aiogram-2.25-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Downloading aiogram-2.24-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Downloading aiogram-2.23.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Downloading aiogram-2.23-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Downloading aiogram-2.22.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Downloading aiogram-2.22.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "INFO: pip is still looking at multiple versions of aiogram to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading aiogram-2.22-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading aiogram-2.21-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading aiogram-2.20-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading aiogram-2.19-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading aiogram-2.18-py3-none-any.whl.metadata (3.5 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading aiogram-2.17.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading aiogram-2.17-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading aiogram-2.16-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Downloading aiogram-2.15-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: Babel>=2.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiogram<2.26->rasa) (2.15.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.0->rasa) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.0->rasa) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.0->rasa) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.0->rasa) (4.0.3)\n",
      "Requirement already satisfied: tzlocal!=3.*,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from apscheduler<3.10,>=3.6->rasa) (5.2)\n",
      "Collecting botocore<1.36.0,>=1.35.76 (from boto3<2.0.0,>=1.26.136->rasa)\n",
      "  Downloading botocore-1.35.76-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.26.136->rasa)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.26.136->rasa)\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting msgpack>=0.5.2 (from CacheControl<0.13.0,>=0.12.9->rasa)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from coloredlogs<16,>=10->rasa) (10.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cryptography>=41.0.7->rasa) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3->rasa) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3->rasa) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3->rasa) (4.9)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 (from jsonschema<4.18,>=3.2->rasa)\n",
      "  Downloading pyrsistent-0.20.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (27 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<3.6,>=3.1->rasa)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<3.6,>=3.1->rasa)\n",
      "  Downloading fonttools-4.55.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (164 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib<3.6,>=3.1->rasa)\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib<3.6,>=3.1->rasa) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib<3.6,>=3.1->rasa) (3.2.0)\n",
      "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from prompt-toolkit<3.0.29,>=3.0->rasa) (0.2.13)\n",
      "Collecting docopt>=0.6.2 (from pykwalify<1.9,>=1.7->rasa)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting simple-websocket>=0.10.0 (from python-engineio!=5.0.0,<6,>=4->rasa)\n",
      "  Downloading simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting bidict>=0.21.0 (from python-socketio<6,>=4.4->rasa)\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting fire (from randomname<0.2.0,>=0.1.5->rasa)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0,>=2.23->rasa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0,>=2.23->rasa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0,>=2.23->rasa) (2.2.1)\n",
      "Collecting ruamel.yaml.clib>=0.2.6 (from ruamel.yaml<0.17.22,>=0.16.5->rasa)\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-macosx_13_0_arm64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: httptools>=0.0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sanic<21.13,>=21.12->rasa) (0.6.4)\n",
      "Collecting aiofiles>=0.6.0 (from sanic<21.13,>=21.12->rasa)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<3.10,>=3.9.0->rasa)\n",
      "  Downloading multidict-5.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: uvloop>=0.5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sanic<21.13,>=21.12->rasa) (0.21.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn<1.2,>=0.22->rasa) (3.5.0)\n",
      "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite<0.4,>=0.3->rasa)\n",
      "  Downloading python_crfsuite-0.9.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: tabulate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sklearn-crfsuite<0.4,>=0.3->rasa) (0.9.0)\n",
      "INFO: pip is looking at multiple versions of structlog-sentry to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting structlog-sentry<3.0.0,>=2.0.2 (from rasa)\n",
      "  Downloading structlog_sentry-2.2.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading structlog_sentry-2.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting aiohttp-retry>=2.8.3 (from twilio<8.3,>=6.26->rasa)\n",
      "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting future (from webexteamssdk<1.7.0,>=1.1.1->rasa)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: requests-toolbelt in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from webexteamssdk<1.7.0,>=1.1.1->rasa) (1.0.0)\n",
      "Collecting pamqp==3.2.1 (from aiormq~=6.4.0->aio-pika<8.2.4,>=6.7.1->rasa)\n",
      "  Downloading pamqp-3.2.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=41.0.7->rasa) (2.22)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jaxlib-0.4.35-cp310-cp310-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting ml-dtypes>=0.4.0 (from jax>=0.3.15->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading ml_dtypes-0.5.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.3.15 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jaxlib-0.4.34-cp310-cp310-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jaxlib-0.4.33-cp310-cp310-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jaxlib-0.4.31-cp310-cp310-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading jaxlib-0.4.30-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting locket (from partd>=0.3.10->dask==2022.10.2->rasa)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3->rasa) (0.6.1)\n",
      "Requirement already satisfied: wsproto in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from simple-websocket>=0.10.0->python-engineio!=5.0.0,<6,>=4->rasa) (1.2.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.13,>=2.12->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.13,>=2.12->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.13,>=2.12->tensorflow-macos==2.12.0->rasa)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl->aio-pika<8.2.4,>=6.7.1->rasa) (0.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos==2.12.0->rasa) (2.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-macos==2.12.0->rasa) (2.1.5)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from wsproto->simple-websocket>=0.10.0->python-engineio!=5.0.0,<6,>=4->rasa) (0.14.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos==2.12.0->rasa) (3.2.2)\n",
      "Downloading rasa-3.6.20-py3-none-any.whl (838 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m838.7/838.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dask-2022.10.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "Downloading tensorflow_macos-2.12.0-cp310-cp310-macosx_12_0_arm64.whl (200.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m200.8/200.8 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Downloading aio_pika-8.2.3-py3-none-any.whl (49 kB)\n",
      "Downloading aiogram-2.15-py3-none-any.whl (184 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-macosx_11_0_arm64.whl (389 kB)\n",
      "Downloading APScheduler-3.9.1.post1-py2.py3-none-any.whl (59 kB)\n",
      "Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "Downloading boto3-1.35.76-py3-none-any.whl (139 kB)\n",
      "Downloading CacheControl-0.12.14-py2.py3-none-any.whl (21 kB)\n",
      "Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading colorhash-1.2.1-py3-none-any.whl (5.7 kB)\n",
      "Downloading confluent_kafka-2.6.1-cp310-cp310-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fbmessenger-6.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Downloading jsonpickle-3.0.4-py3-none-any.whl (39 kB)\n",
      "Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Downloading matplotlib-3.5.3-cp310-cp310-macosx_11_0_arm64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
      "Downloading protobuf-4.23.3-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp310-cp310-macosx_14_0_arm64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.9-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading pymongo-4.3.3-cp310-cp310-macosx_10_9_universal2.whl (413 kB)\n",
      "Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Downloading python_engineio-4.10.1-py3-none-any.whl (57 kB)\n",
      "Downloading python_socketio-5.11.4-py3-none-any.whl (76 kB)\n",
      "Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "Downloading questionary-1.10.0-py3-none-any.whl (31 kB)\n",
      "Downloading rasa_sdk-3.6.2-py3-none-any.whl (45 kB)\n",
      "Downloading redis-4.6.0-py3-none-any.whl (241 kB)\n",
      "Downloading regex-2022.10.31-cp310-cp310-macosx_11_0_arm64.whl (287 kB)\n",
      "Downloading rocketchat_API-1.30.0-py3-none-any.whl (21 kB)\n",
      "Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
      "Downloading sanic-21.12.2-py3-none-any.whl (156 kB)\n",
      "Downloading Sanic_Cors-2.0.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading sanic_jwt-1.8.0-py3-none-any.whl (23 kB)\n",
      "Downloading sanic_routing-0.7.2-py3-none-any.whl (23 kB)\n",
      "Downloading scikit_learn-1.1.3-cp310-cp310-macosx_12_0_arm64.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-macosx_12_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Downloading slack_sdk-3.33.5-py2.py3-none-any.whl (292 kB)\n",
      "Downloading structlog-23.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading structlog_sentry-2.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading tarsafe-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Downloading twilio-8.2.2-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
      "Downloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl (51 kB)\n",
      "Downloading webexteamssdk-1.6.1-py3-none-any.whl (113 kB)\n",
      "Downloading websockets-10.4-cp310-cp310-macosx_11_0_arm64.whl (97 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
      "Downloading aiormq-6.4.2-py3-none-any.whl (34 kB)\n",
      "Downloading pamqp-3.2.1-py2.py3-none-any.whl (33 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Downloading botocore-1.35.76-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.2-cp310-cp310-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.12.1-cp310-cp310-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp310-cp310-macosx_11_0_arm64.whl (64 kB)\n",
      "Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-macosx_11_0_arm64.whl (81 kB)\n",
      "Downloading multidict-5.2.0-cp310-cp310-macosx_11_0_arm64.whl (45 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading pyrsistent-0.20.0-cp310-cp310-macosx_10_9_universal2.whl (83 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp310-cp310-macosx_11_0_arm64.whl (319 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-macosx_13_0_arm64.whl (131 kB)\n",
      "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Downloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading wrapt-1.14.1-cp310-cp310-macosx_11_0_arm64.whl (35 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading jaxlib-0.4.30-cp310-cp310-macosx_11_0_arm64.whl (66.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading ml_dtypes-0.5.0-cp310-cp310-macosx_10_9_universal2.whl (736 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m736.8/736.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: mattermostwrapper, randomname, SQLAlchemy, docopt, fire\n",
      "  Building wheel for mattermostwrapper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mattermostwrapper: filename=mattermostwrapper-2.2-py3-none-any.whl size=2448 sha256=eefc212c62e351d7776698cbd62bd50df86e23e4183f068bc1645d747a0ad68a\n",
      "  Stored in directory: /Users/jimenaames/Library/Caches/pip/wheels/b2/dc/02/e3239f0ea0a676085826846d32ef09b10915c0a33c817d2dbb\n",
      "  Building wheel for randomname (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for randomname: filename=randomname-0.1.5-py3-none-any.whl size=58808 sha256=92de74e72df2e4da88a588e27984d703dbdf76b8ef1314b27f45f0ead7272c40\n",
      "  Stored in directory: /Users/jimenaames/Library/Caches/pip/wheels/ee/df/c1/0bdf17c694217f49657ca36fd0239b9a243c34c27a70ff56ef\n",
      "  Building wheel for SQLAlchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.4.54-cp310-cp310-macosx_10_9_universal2.whl size=1589004 sha256=afe0cc7a7bc899afde93f2d84abbc832954843d7397c725bc94d78c7ede4e966\n",
      "  Stored in directory: /Users/jimenaames/Library/Caches/pip/wheels/67/1e/e3/21b7fd749e112fdcd46ea531c749344394b2c07c0a39da8e3a\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e6a54dca587bd546d138ca5d5c4cfb509927382d28cac30321ca6a2cd74e7c99\n",
      "  Stored in directory: /Users/jimenaames/Library/Caches/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=3b81f282c8ca33e97366d5202d5c37e5fe3ba6e002773b69cfef8d110e1ce61d\n",
      "  Stored in directory: /Users/jimenaames/Library/Caches/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built mattermostwrapper randomname SQLAlchemy docopt fire\n",
      "Installing collected packages: sanic-routing, pytz, libclang, docopt, wrapt, werkzeug, websockets, ujson, typing-utils, toolz, terminaltables, termcolor, tensorflow-estimator, tensorboard-data-server, tarsafe, structlog, SQLAlchemy, slack-sdk, setuptools, sentry-sdk, ruamel.yaml.clib, regex, redis, python-dateutil, python-crfsuite, pyrsistent, PyJWT, pydot, pydantic, psycopg2-binary, protobuf, prompt-toolkit, portalocker, pluggy, pamqp, packaging, opt-einsum, numpy, networkx, multidict, msgpack, markdown, locket, kiwisolver, keras, jsonpickle, joblib, jmespath, google-pasta, gast, future, fonttools, dnspython, cycler, confluent-kafka, colorhash, cloudpickle, bidict, attrs, astunparse, aiofiles, absl-py, tensorflow_hub, structlog-sentry, sklearn-crfsuite, simple-websocket, scipy, sanic-jwt, sanic, ruamel.yaml, rocketchat_API, questionary, pymongo, partd, ml-dtypes, mattermostwrapper, matplotlib, jsonschema, h5py, fire, fbmessenger, CacheControl, botocore, apscheduler, webexteamssdk, scikit-learn, sanic-cors, s3transfer, randomname, python-engineio, pykwalify, jaxlib, google-auth-oauthlib, dask, aiormq, aiohttp, tensorboard, rasa-sdk, python-socketio, jax, boto3, aiohttp-retry, aiogram, aio-pika, twilio, tensorflow-macos, rasa\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.2\n",
      "    Uninstalling pytz-2024.2:\n",
      "      Successfully uninstalled pytz-2024.2\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 14.0\n",
      "    Uninstalling websockets-14.0:\n",
      "      Successfully uninstalled websockets-14.0\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.15\n",
      "    Uninstalling SQLAlchemy-2.0.15:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.15\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 63.2.0\n",
      "    Uninstalling setuptools-63.2.0:\n",
      "      Successfully uninstalled setuptools-63.2.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.19\n",
      "    Uninstalling pydantic-1.10.19:\n",
      "      Successfully uninstalled pydantic-1.10.19\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.43\n",
      "    Uninstalling prompt-toolkit-3.0.43:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.43\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.4.2\n",
      "    Uninstalling networkx-3.4.2:\n",
      "      Successfully uninstalled networkx-3.4.2\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.1.0\n",
      "    Uninstalling multidict-6.1.0:\n",
      "      Successfully uninstalled multidict-6.1.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.2.0\n",
      "    Uninstalling attrs-23.2.0:\n",
      "      Successfully uninstalled attrs-23.2.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "    Uninstalling scipy-1.13.1:\n",
      "      Successfully uninstalled scipy-1.13.1\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.22.0\n",
      "    Uninstalling jsonschema-4.22.0:\n",
      "      Successfully uninstalled jsonschema-4.22.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.2\n",
      "    Uninstalling scikit-learn-1.5.2:\n",
      "      Successfully uninstalled scikit-learn-1.5.2\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.10.10\n",
      "    Uninstalling aiohttp-3.10.10:\n",
      "      Successfully uninstalled aiohttp-3.10.10\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 8.24.0 requires prompt-toolkit<3.1.0,>=3.0.41, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
      "jupyter-console 6.6.3 requires prompt-toolkit>=3.0.30, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
      "jupyter-events 0.10.0 requires jsonschema[format-nongpl]>=4.18.0, but you have jsonschema 4.17.3 which is incompatible.\n",
      "jupyter-server 2.14.0 requires packaging>=22.0, but you have packaging 20.9 which is incompatible.\n",
      "jupyterlab-server 2.27.2 requires jsonschema>=4.18.0, but you have jsonschema 4.17.3 which is incompatible.\n",
      "jupyterlab-server 2.27.2 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
      "langchain 0.3.8 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.9 which is incompatible.\n",
      "langchain-core 0.3.21 requires packaging<25,>=23.2, but you have packaging 20.9 which is incompatible.\n",
      "langchain-core 0.3.21 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.9 which is incompatible.\n",
      "localai 0.0.26 requires sentence-transformers==2.2.2, but you have sentence-transformers 3.3.1 which is incompatible.\n",
      "localai 0.0.26 requires SQLAlchemy==2.0.15, but you have sqlalchemy 1.4.54 which is incompatible.\n",
      "opentelemetry-proto 1.28.1 requires protobuf<6.0,>=5.0, but you have protobuf 4.23.3 which is incompatible.\n",
      "pydantic-settings 2.6.1 requires pydantic>=2.7.0, but you have pydantic 1.10.9 which is incompatible.\n",
      "referencing 0.35.1 requires attrs>=22.2.0, but you have attrs 22.1.0 which is incompatible.\n",
      "trio 0.26.0 requires attrs>=23.2.0, but you have attrs 22.1.0 which is incompatible.\n",
      "unstructured-client 0.25.9 requires packaging>=23.1, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed CacheControl-0.12.14 PyJWT-2.10.1 SQLAlchemy-1.4.54 absl-py-1.4.0 aio-pika-8.2.3 aiofiles-24.1.0 aiogram-2.15 aiohttp-3.9.5 aiohttp-retry-2.9.1 aiormq-6.4.2 apscheduler-3.9.1.post1 astunparse-1.6.3 attrs-22.1.0 bidict-0.23.1 boto3-1.35.76 botocore-1.35.76 cloudpickle-2.2.1 colorhash-1.2.1 confluent-kafka-2.6.1 cycler-0.12.1 dask-2022.10.2 dnspython-2.3.0 docopt-0.6.2 fbmessenger-6.0.0 fire-0.7.0 fonttools-4.55.2 future-1.0.0 gast-0.4.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 h5py-3.12.1 jax-0.4.30 jaxlib-0.4.30 jmespath-1.0.1 joblib-1.2.0 jsonpickle-3.0.4 jsonschema-4.17.3 keras-2.12.0 kiwisolver-1.4.7 libclang-18.1.1 locket-1.0.0 markdown-3.7 matplotlib-3.5.3 mattermostwrapper-2.2 ml-dtypes-0.5.0 msgpack-1.1.0 multidict-5.2.0 networkx-2.6.3 numpy-1.23.5 opt-einsum-3.4.0 packaging-20.9 pamqp-3.2.1 partd-1.4.2 pluggy-1.5.0 portalocker-2.10.1 prompt-toolkit-3.0.28 protobuf-4.23.3 psycopg2-binary-2.9.10 pydantic-1.10.9 pydot-1.4.2 pykwalify-1.8.0 pymongo-4.3.3 pyrsistent-0.20.0 python-crfsuite-0.9.11 python-dateutil-2.8.2 python-engineio-4.10.1 python-socketio-5.11.4 pytz-2022.7.1 questionary-1.10.0 randomname-0.1.5 rasa-3.6.20 rasa-sdk-3.6.2 redis-4.6.0 regex-2022.10.31 rocketchat_API-1.30.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.12 s3transfer-0.10.4 sanic-21.12.2 sanic-cors-2.0.1 sanic-jwt-1.8.0 sanic-routing-0.7.2 scikit-learn-1.1.3 scipy-1.10.1 sentry-sdk-1.14.0 setuptools-75.6.0 simple-websocket-1.1.0 sklearn-crfsuite-0.3.6 slack-sdk-3.33.5 structlog-23.3.0 structlog-sentry-2.1.0 tarsafe-0.0.4 tensorboard-2.12.3 tensorboard-data-server-0.7.2 tensorflow-estimator-2.12.0 tensorflow-macos-2.12.0 tensorflow_hub-0.13.0 termcolor-2.5.0 terminaltables-3.1.10 toolz-1.0.0 twilio-8.2.2 typing-utils-0.1.0 ujson-5.10.0 webexteamssdk-1.6.1 websockets-10.4 werkzeug-3.1.3 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "%pip install -U langchain-community\n",
    "%pip install openai==0.28\n",
    "%pip install python-dotenv\n",
    "%pip install pydantic\n",
    "\n",
    "%pip install pickle5\n",
    "%pip install numpy\n",
    "%pip install sentence-transformers\n",
    "%pip install chromadb\n",
    "%pip install rasa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found something in the **description** category:\n",
      " **Details**: Small cracks radiating from the gate, in the thick unorientated material in the\n",
      "centre of the base or in the partially unorientated material of the straps evident\n",
      "after carbonation. Severe stress cracking on every foot and strap centred\n",
      "indicates a chemical reaction.\n",
      "The bottle may explode after a period of time or at high temperatures once\n",
      "carbonated.\n",
      " **Source**: organized_pet_bottle_faults_df\n",
      " **Relevance Score**: 0.57\n",
      "\n",
      "I found something in the **description** category:\n",
      " **Details**: Break starts around the gate. Almost no distortion of the bottle before break\n",
      "occurs. May have excessive crystalinity at the gate.\n",
      " **Source**: organized_pet_bottle_faults_df\n",
      " **Relevance Score**: 0.51\n",
      "\n",
      "I found something in the **description** category:\n",
      " **Details**: The bottle breaks through the body often tearing the bottle apart. Often showing\n",
      "excessive pressure distortion of the base.\n",
      " **Source**: organized_pet_bottle_faults_df\n",
      " **Relevance Score**: 0.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#works! \n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class BackendForQuerying:\n",
    "    def __init__(self, db_file='vector_database.pkl', model_name='all-MiniLM-L6-v2'):\n",
    "        # Load the vector database from file\n",
    "        with open(db_file, 'rb') as file:\n",
    "            vector_database = pickle.load(file)\n",
    "        self.embeddings = vector_database['embeddings']\n",
    "        self.metadata = vector_database['metadata']\n",
    "        \n",
    "        # Initialize the model for embeddings\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def query(self, query_text, n_results=2):\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.model.encode([query_text])[0]  # Take the first result\n",
    "        \n",
    "        # Calculate cosine similarity with database embeddings\n",
    "        similarities = [\n",
    "            (i, np.dot(query_embedding, embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(embedding)))\n",
    "            for i, embedding in enumerate(self.embeddings)\n",
    "        ]\n",
    "        \n",
    "        # Sort results by similarity score\n",
    "        top_results = sorted(similarities, key=lambda x: x[1], reverse=True)[:n_results]\n",
    "        \n",
    "        # Format results\n",
    "        responses = []\n",
    "        for idx, score in top_results:\n",
    "            meta = self.metadata[idx]\n",
    "            response = {\n",
    "                \"score\": score,\n",
    "                \"source\": meta.get('source', 'Unknown source'),\n",
    "                \"column\": meta.get('column', 'Unknown column'),\n",
    "                \"original_text\": meta.get('original_text', 'No detailed information available.'),\n",
    "            }\n",
    "            responses.append(response)\n",
    "        return responses\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    backend = BackendForQuerying()\n",
    "    query_text = \"how to make water bottle lid not crack\"\n",
    "    results = backend.query(query_text, n_results=3)\n",
    "    \n",
    "    # Print results in chatbot-friendly format\n",
    "    for result in results:\n",
    "        print(\n",
    "            f\"I found something in the **{result['column']}** category:\\n\"\n",
    "            f\" **Details**: {result['original_text']}\\n\"\n",
    "            f\" **Source**: {result['source']}\\n\"\n",
    "            f\" **Relevance Score**: {result['score']:.2f}\\n\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackendForQuerying\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, db_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_database.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Load vector database\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(db_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[28], line 58\u001b[0m, in \u001b[0;36mBackendForQuerying\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m.\u001b[39minput_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow to prevent water bottle lids from cracking?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_text, n_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Retrieve relevant documents\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 54\u001b[0m, in \u001b[0;36mBackendForQuerying.generate_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(prompt):\n\u001b[0;32m---> 54\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m.\u001b[39minput_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#idk why this doesn't work either \n",
    "import pickle\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class BackendForQuerying:\n",
    "    def __init__(self, db_file='vector_database.pkl', model_name='all-MiniLM-L6-v2'):\n",
    "        # Load vector database\n",
    "        with open(db_file, 'rb') as file:\n",
    "            vector_database = pickle.load(file)\n",
    "        self.embeddings = vector_database['embeddings']\n",
    "        self.metadata = vector_database['metadata']\n",
    "        \n",
    "        # Initialize sentence transformer modelgit add\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Initialize OpenAI API key\n",
    "        \n",
    "\n",
    "    def query_vector_database(self, query_text, n_results=2):\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.model.encode([query_text])[0]\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarities = [\n",
    "            (i, np.dot(query_embedding, embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(embedding)))\n",
    "            for i, embedding in enumerate(self.embeddings)\n",
    "        ]\n",
    "        \n",
    "        # Retrieve top results\n",
    "        top_results = sorted(similarities, key=lambda x: x[1], reverse=True)[:n_results]\n",
    "        \n",
    "        retrieved_docs = []\n",
    "        for idx, score in top_results:\n",
    "            meta = self.metadata[idx]\n",
    "            retrieved_docs.append({\n",
    "                \"text\": meta.get('original_text', 'No detailed information available.'),\n",
    "                \"source\": meta.get('source', 'Unknown source'),\n",
    "                \"score\": score\n",
    "            })\n",
    "        return retrieved_docs\n",
    "\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    # Load a pre-trained language model\n",
    "    model_name = \"gpt2\"  # Replace with a free model from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Generate a response\n",
    "    def generate_response(prompt):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs.input_ids, max_length=100, num_return_sequences=1)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    response = generate_response(\"How to prevent water bottle lids from cracking?\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def handle_query(self, query_text, n_results=2):\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.query_vector_database(query_text, n_results)\n",
    "        \n",
    "        # Generate response using retrieved documents\n",
    "        response = self.generate_response(query_text, retrieved_docs)\n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    backend = BackendForQuerying()\n",
    "    user_query = \"How to prevent water bottle lids from cracking?\"\n",
    "    chatbot_response = backend.handle_query(user_query, n_results=3)\n",
    "    print(f\"Chatbot Response:\\n{chatbot_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_validator' from 'pydantic' (/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/__init__.cpython-310-darwin.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from langchain.llms import openai\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain/vectorstores/__init__.py:24\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"**Vector store** stores embedded data and performs vector search.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mOne of the most common ways to store and search over unstructured data is to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Embeddings, Document\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStore\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_importer\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/vectorstores/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VST, VectorStore, VectorStoreRetriever\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01min_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryVectorStore\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorStore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorStoreRetriever\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInMemoryVectorStore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/vectorstores/base.py:39\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycle\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     32\u001b[0m     Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     TypeVar,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigDict, Field, model_validator\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embeddings\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRetriever, LangSmithRetrieverParams\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'model_validator' from 'pydantic' (/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/__init__.cpython-310-darwin.so)"
     ]
    }
   ],
   "source": [
    "#ignore this work LOL\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.llms import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found in .env file.\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Hello, world!\",\n",
    "    max_tokens=5\n",
    ")\n",
    "print(response)\n",
    "\n",
    "\n",
    "class VectorDatabaseHandler:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', persist_directory='vector_db'):\n",
    "        # Initialize the SentenceTransformer model\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        # Set up Chroma vector store\n",
    "        self.embedding_function = SentenceTransformerEmbeddingFunction(self.model)\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=\"document_collection\",\n",
    "            embedding_function=self.embedding_function,\n",
    "            persist_directory=persist_directory  # Directory for persistent Chroma storage\n",
    "        )\n",
    "\n",
    "    def load_vector_database(self, filename='vector_database.pkl'):\n",
    "\n",
    "        \"\"\"Load the saved vector database from a pickle file.\"\"\"\n",
    "        with open(filename, 'rb') as file:\n",
    "            vector_database = pickle.load(file)\n",
    "        print(f\"Vector database loaded from {filename}\")\n",
    "\n",
    "        return vector_database['embeddings'], vector_database['metadata']\n",
    "\n",
    "    def save_vector_database(self, embeddings_list, metadata_list, filename='vector_database.pkl'):\n",
    "        \"\"\"Save embeddings and metadata to a pickle file.\"\"\"\n",
    "        vector_database = {\n",
    "            'embeddings': embeddings_list,\n",
    "            'metadata': metadata_list\n",
    "        }\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(vector_database, file)\n",
    "        print(f\"Vector database saved to {filename}\")\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        return self.model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    def populate_vector_store_from_saved_data(self, filename='vector_database.pkl'):\n",
    "        \"\"\"Populate the vector store with data loaded from the saved vector database.\"\"\"\n",
    "\n",
    "        try: \n",
    "            # Load data from the saved pickle file\n",
    "            embeddings, metadata = self.load_vector_database(filename)\n",
    "            embeddings = [\n",
    "                np.nan_to_num(embedding, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                for embedding in embeddings\n",
    "            ]\n",
    "\n",
    "            # Ensure embeddings are in the correct format (list of lists)\n",
    "            if isinstance(embeddings, np.ndarray):\n",
    "                embeddings = embeddings.tolist()  # Convert to list if it's a numpy array\n",
    "            elif not isinstance(embeddings, list):\n",
    "                raise ValueError(\"Embeddings should be a list or numpy array.\")\n",
    "\n",
    "            # Ensure embeddings are not empty\n",
    "            if isinstance(embeddings, np.ndarray):  # If embeddings is a NumPy array\n",
    "                if len(embeddings)== 0:\n",
    "                    raise ValueError(\"Embeddings are empty.\")\n",
    "            elif isinstance(embeddings, list):  # If embeddings is a list\n",
    "                if len(embeddings) == 0:\n",
    "                    raise ValueError(\"Embeddings are empty.\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown type for embeddings. Expected list or numpy.ndarray.\")\n",
    "\n",
    "            # old version of flattening\n",
    "            # embeddings = [embedding if isinstance(embedding, list) else embedding.tolist() for embedding in embeddings]\n",
    "            # new version of flattering : ensure each embedding is a list (flattening if necessary)\n",
    "            embeddings = [embedding.flatten().tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in embeddings]\n",
    "\n",
    "            # Ensure embeddings are not empty\n",
    "            if isinstance(embeddings, np.ndarray):  # If embeddings is a NumPy array\n",
    "                if embeddings.size == 0:  # Correct way to check for emptiness\n",
    "                    raise ValueError(\"Embeddings are empty.\")\n",
    "            elif isinstance(embeddings, list):  # If embeddings is a list\n",
    "                if len(embeddings) == 0:\n",
    "                    raise ValueError(\"Embeddings are empty.\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown type for embeddings. Expected list or numpy.ndarray.\")\n",
    "\n",
    "\n",
    "            # Ensure metadata is correctly structured\n",
    "            if not metadata:\n",
    "                raise ValueError(\"Metadata is empty.\")\n",
    "\n",
    "            # Ensures original text exists\n",
    "            texts = [meta['original_text'] for meta in metadata if 'original_text' in meta]\n",
    "            if not texts:\n",
    "                raise ValueError(\"No 'original_text' key found in metadata.\")\n",
    "\n",
    "            # Ensures embeddings are numeric\n",
    "            for idx, embedding in enumerate(embeddings):\n",
    "                if not all(isinstance(val, (int, float, np.floating, np.integer)) for val in embedding):\n",
    "                    raise ValueError(f\"Embedding at index {idx} contains non-numeric values: {embedding}\")\n",
    "\n",
    "            # Add embeddings and metadata to the vector store\n",
    "            self.vector_store.add_texts(\n",
    "                texts=[meta['original_text'] for meta in metadata],  # Assuming original_text is part of the metadata\n",
    "                metadatas=metadata,\n",
    "                embeddings=embeddings  # Now we directly pass embeddings\n",
    "            )\n",
    "\n",
    "            # Add texts and embeddings one by one to debug\n",
    "            # for i in range(len(metadata)):\n",
    "            #     try:\n",
    "            #         self.vector_store.add_texts(\n",
    "            #             texts=[metadata[i]['original_text']],  # Add a single text at a time\n",
    "            #             metadatas=[metadata[i]],  # Add the corresponding metadata\n",
    "            #             embeddings=[embeddings[i]]  # Add a single embedding\n",
    "            #         )\n",
    "            #     except Exception as e:\n",
    "            #         print(f\"Error adding item {i}: {e}\")\n",
    "\n",
    "            self.vector_store.persist()\n",
    "            print(\"Vector store populated with data from saved file.\")\n",
    "\n",
    "            # debugging the data structure\n",
    "            print(f\"Embeddings type: {type(embeddings)}, length: {len(embeddings)}\")\n",
    "            print(f\"Metadata type: {type(metadata[:3])}, length: {len(metadata)}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"ValueError occured: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occured: {e}\")\n",
    "\n",
    "        # test the data after loading\n",
    "        print(f\"Loaded embeddings: {embeddings[:3]}\")  # Preview first 3 embeddings\n",
    "        print(f\"Loaded metadata: {metadata[:3]}\")    # Preview first 3 metadata entries\n",
    "        print(f\"Embeddings preview: {embeddings[:3]}\")  # Check first 3 embeddings\n",
    "        # does not print so len of embeddings and metadata match so not the problem\n",
    "        if len(embeddings) != len(metadata):\n",
    "            raise ValueError(f\"Number of embeddings ({len(embeddings)}) does not match number of metadata ({len(metadata)}).\")\n",
    "        # both print class list so that's not the problem\n",
    "        print(f\"Type of embeddings: {type(embeddings)}\")\n",
    "        print(f\"Type of metadata: {type(metadata)}\")\n",
    "        \n",
    "    def handle_query(self, user_query, k=2):\n",
    "        \"\"\"Handles a user query by retrieving relevant documents and generating a response.\"\"\"\n",
    "        # Generate embeddings for the query\n",
    "        query_embedding = self.generate_embeddings(user_query)\n",
    "\n",
    "        # Debugging: Validate query_embedding\n",
    "        print(f\"query_embedding type: {type(query_embedding)}, shape: {getattr(query_embedding, 'shape', 'Not a numpy array')}\")\n",
    "        print(f\"query_embedding: {query_embedding}\")\n",
    "\n",
    "        # Flatten the embedding if necessary\n",
    "        if isinstance(query_embedding, np.ndarray) and query_embedding.ndim == 2 and query_embedding.shape[0] == 1:\n",
    "            query_embedding = query_embedding.flatten()\n",
    "\n",
    "        # Ensure it's a valid 1D array\n",
    "        if not isinstance(query_embedding, np.ndarray) or query_embedding.ndim != 1:\n",
    "            raise ValueError(f\"Invalid query embedding: {query_embedding}\")\n",
    "\n",
    "        # Perform similarity search\n",
    "        # results = self.vector_store.similarity_search(query=query_embedding, k=k)\n",
    "        results = self.vector_store.similarity_search_by_vector(embedding=query_embedding, k=k)\n",
    "\n",
    "        # Extract relevant documents\n",
    "        retrieved_docs = [result['text'] for result in results]\n",
    "        retrieved_metadata = [result['metadata'] for result in results]\n",
    "\n",
    "        # Debugging: Check results\n",
    "        print(f\"Results: {results}\")\n",
    "        print(f\"Retrieved documents: {retrieved_docs}\")\n",
    "\n",
    "        # Construct a prompt for response generation\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant. Below are some relevant documents retrieved based on a user's query.\n",
    "        Use this information to generate a concise and helpful response.\n",
    "\n",
    "        Relevant Documents:\n",
    "        {retrieved_docs}\n",
    "\n",
    "        User Query:\n",
    "        {user_query}\n",
    "\n",
    "        Your Response:\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response\n",
    "\n",
    "        \n",
    "        # Generate response using LLM\n",
    "        # llm = openai(model_name=\"gpt-4\")  # Replace with your LLM setup\n",
    "        # response = llm(prompt)\n",
    "        # return response\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        # try:\n",
    "        #     # Generate embeddings for the query\n",
    "        #     query_embedding = self.generate_embeddings(user_query)\n",
    "\n",
    "        #     # Debugging: Validate query_embedding\n",
    "        #     print(f\"query_embedding type: {type(query_embedding)}, shape: {getattr(query_embedding, 'shape', 'Not a numpy array')}\")\n",
    "        #     print(f\"query_embedding: {query_embedding}\")\n",
    "\n",
    "        #     # Flatten the embedding if necessary\n",
    "        #     if isinstance(query_embedding, np.ndarray) and query_embedding.ndim == 2 and query_embedding.shape[0] == 1:\n",
    "        #         query_embedding = query_embedding.flatten()\n",
    "\n",
    "        #     # Ensure it's a valid 1D array\n",
    "        #     if not isinstance(query_embedding, np.ndarray) or query_embedding.ndim != 1:\n",
    "        #         raise ValueError(f\"Invalid query embedding: {query_embedding}\")\n",
    "\n",
    "        #     # Perform similarity search\n",
    "        #     results = self.vector_store.similarity_search(query=query_embedding, k=k)\n",
    "\n",
    "        #     # Extract relevant documents\n",
    "        #     retrieved_docs = [result['text'] for result in results]\n",
    "        #     retrieved_metadata = [result['metadata'] for result in results]\n",
    "\n",
    "        #     # Debugging: Check results\n",
    "        #     print(f\"Results: {results}\")\n",
    "        #     print(f\"Retrieved documents: {retrieved_docs}\")\n",
    "\n",
    "        #     # Construct a prompt for response generation\n",
    "        #     prompt = f\"\"\"\n",
    "        #     You are an AI assistant. Below are some relevant documents retrieved based on a user's query.\n",
    "        #     Use this information to generate a concise and helpful response.\n",
    "\n",
    "        #     Relevant Documents:\n",
    "        #     {retrieved_docs}\n",
    "\n",
    "        #     User Query:\n",
    "        #     {user_query}\n",
    "\n",
    "        #     Your Response:\n",
    "        #     \"\"\"\n",
    "\n",
    "        #     # Generate response using LLM\n",
    "        #     llm = openai(model_name=\"gpt-4\")  # Replace with your LLM setup\n",
    "        #     response = llm(prompt)\n",
    "        #     return response\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error during query handling: {e}\")\n",
    "        #     return f\"An error occurred: {e}\"\n",
    "\n",
    "        \n",
    "    # def handle_query(self, user_query, k=2):\n",
    "    #     query_embedding = self.generate_embeddings(user_query)\n",
    "\n",
    "    #     # Validate the embedding\n",
    "    #     if not isinstance(query_embedding, np.ndarray) or query_embedding.ndim != 1:\n",
    "    #         raise ValueError(f\"Invalid query embedding: {query_embedding}\")\n",
    "\n",
    "    #     # Perform similarity search in the vector database\n",
    "    #     results = self.vector_store.similarity_search(query=query_embedding, k=k)\n",
    "\n",
    "    #     # Ensure dimensionality matches the stored vectors\n",
    "    #     stored_vector_dim = self.vector_store.vector_dim  # Assuming Chroma provides this\n",
    "    #     if query_embedding.shape[0] != stored_vector_dim:\n",
    "    #         raise ValueError(f\"Dimensionality mismatch: expected {stored_vector_dim}, got {query_embedding.shape[0]}\")\n",
    "\n",
    "    #     # Perform similarity search in the vector database\n",
    "    #     results = self.vector_store.similarity_search(query=query_embedding, k=k)\n",
    "\n",
    "    #     # Extract relevant documents and metadata\n",
    "    #     retrieved_docs = [result['text'] for result in results]\n",
    "    #     retrieved_metadata = [result['metadata'] for result in results]\n",
    "\n",
    "    #     # Construct a prompt for response generation\n",
    "    #     prompt = f\"\"\"\n",
    "    #     You are an AI assistant. Below are some relevant documents retrieved based on a user's query.\n",
    "    #     Use this information to generate a concise and helpful response.\n",
    "\n",
    "    #     Relevant Documents:\n",
    "    #     {retrieved_docs}\n",
    "\n",
    "    #     User Query:\n",
    "    #     {user_query}\n",
    "\n",
    "    #     Your Response:\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     # Generate response using LLM\n",
    "    #     llm = openai(model_name=\"gpt-4\")  # Replace with your LLM setup\n",
    "    #     response = llm(prompt)\n",
    "    #     return response\n",
    "\n",
    "\n",
    "    # def handle_query(self, user_query, k=2):\n",
    "    #     \"\"\"Handles a user query by retrieving relevant documents and generating a response.\"\"\"\n",
    "    #     query_embedding = self.vector_store.embed_query(user_query)\n",
    "        \n",
    "    #     if not isinstance(query_embedding, np.ndarray) or query_embedding.ndim != 1:\n",
    "    #         raise ValueError(f\"Invalid query embedding: {query_embedding}\")\n",
    "        \n",
    "    #     # Perform similarity search in the vector database\n",
    "    #     results = self.vector_store.similarity_search(\n",
    "    #         query=user_query,\n",
    "    #         k=k  # Number of relevant documents to retrieve\n",
    "    #     )\n",
    "\n",
    "    #     # Extract relevant documents and metadata\n",
    "    #     retrieved_docs = [result['text'] for result in results]\n",
    "    #     retrieved_metadata = [result['metadata'] for result in results]\n",
    "\n",
    "    #     # Construct a simple prompt without using PromptTemplate\n",
    "    #     prompt = f\"\"\"\n",
    "    #     You are an AI assistant. Below are some relevant documents retrieved based on a user's query.\n",
    "    #     Use this information to generate a concise and helpful response.\n",
    "\n",
    "    #     Relevant Documents:\n",
    "    #     {retrieved_docs}\n",
    "\n",
    "    #     User Query:\n",
    "    #     {user_query}\n",
    "\n",
    "    #     Your Response:\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     # Initialize an LLM (e.g., OpenAI's GPT) for response generation\n",
    "    #     llm = openai(model_name=\"gpt-4\")  # Replace with your preferred LLM model or API key setup\n",
    "\n",
    "    #     # Generate the response\n",
    "    #     response = llm(prompt)\n",
    "    #     return response\n",
    "\n",
    "class SentenceTransformerEmbeddingFunction:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    # Embedding method for documents\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "        return embeddings\n",
    "\n",
    "    # Embedding method for queries\n",
    "    def embed_query(self, query):\n",
    "        embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    openai.api_key = api_key\n",
    "    # Initialize VectorDatabaseHandler\n",
    "    vector_db_handler = VectorDatabaseHandler()\n",
    "\n",
    "    # Step 1: Populate the vector store with the data from the saved vector database\n",
    "    print(\"starting vector population\")\n",
    "    vector_db_handler.populate_vector_store_from_saved_data()\n",
    "\n",
    "    # Step 2: Define a function to process a user query and generate a response\n",
    "    print(\"starting query\")\n",
    "    user_query = \"how to make chicken\"\n",
    "    response = vector_db_handler.handle_query(user_query)\n",
    "\n",
    "    # Step 3: Display the response\n",
    "    print(f\"User Query: {user_query}\")\n",
    "    print(f\"AI Response:\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
